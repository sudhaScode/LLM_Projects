{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrival Augmented Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load env\n",
    "import dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama Index\n",
    "from llama_index import (\n",
    "    VectorStoreIndex, \n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    "    )\n",
    "\n",
    "# check if storage alrady exits\n",
    "PERSIST_DIR =\"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the data and create index\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents=documents)\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the exitiong index\n",
    "    storage_context= StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index=load_index_from_storage(storage_context=storage_context)\n",
    "\n",
    "\n",
    "#documents = SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 57/57 [00:00<00:00, 91.17it/s] \n",
      "Generating embeddings: 100%|██████████| 129/129 [00:04<00:00, 26.68it/s]\n"
     ]
    }
   ],
   "source": [
    "#index= VectorStoreIndex.from_documents(documents=documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x12da9d84940>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine=index.as_query_engine()\n",
    "query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"How the attention works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The attention mechanism in the Transformer model is based on a compatibility function between a query and a set of key-value pairs. The input consists of queries and keys of dimension dk, and values of dimension dv. The attention function computes the dot products of the queries with the keys, scales the dot products by dividing them by the square root of the dimension dk, and applies a softmax function to obtain the weights assigned to each value. The output is then computed as a weighted sum of the values, where the weights are the attention weights. This allows the model to focus on different parts of the input sequence when making predictions.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Attention is a mechanism used in sequence transduction models that allows the model to focus on specific parts of the input sequence when generating the output. It involves mapping a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values. The weights are determined by a compatibility function between the query and the corresponding key.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=query_engine.query(\"What is Attention\")\n",
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Attention is a mechanism used in sequence transduction\n",
      "models that allows the model to focus on specific parts of the input\n",
      "sequence when generating the output. It involves mapping a query and a\n",
      "set of key-value pairs to an output, where the output is computed as a\n",
      "weighted sum of the values. The weights are determined by a\n",
      "compatibility function between the query and the corresponding key.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: 8a58ccf2-81a7-4652-a57f-326bce8109fd\n",
      "Similarity: 0.8079673385653757\n",
      "Text: Attention Is All You Need Ashish Vaswani∗ Google Brain\n",
      "avaswani@google.comNoam Shazeer∗ Google Brain noam@google.comNiki\n",
      "Parmar∗ Google Research nikip@google.comJakob Uszkoreit∗ Google\n",
      "Research usz@google.com Llion Jones∗ Google Research\n",
      "llion@google.comAidan N. Gomez∗† University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗ Google Brain lukasz...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: c361de7d-832a-4be3-bf9a-751c93ae744d\n",
      "Similarity: 0.7927976879089179\n",
      "Text: Figure 1: The Transformer - model architecture. wise fully\n",
      "connected feed-forward network. We employ a residual connection [ 10]\n",
      "around each of the two sub-layers, followed by layer normalization [\n",
      "1]. That is, the output of each sub-layer is LayerNorm( x+ Sublayer(\n",
      "x)), where Sublayer(x)is the function implemented by the sub-layer\n",
      "itself. To fa...\n",
      "Attention is a mechanism used in sequence transduction models that allows the model to focus on specific parts of the input sequence when generating the output. It involves mapping a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values. The weights are determined by a compatibility function between the query and the corresponding key.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.response.pprint_utils import pprint_response\n",
    "pprint_response(response, show_source=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=4)\n",
    "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.80)\n",
    "\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever, node_postprocessors=[postprocessor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"how to best of understanding the attention?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: The best way to understand attention is to think of it\n",
      "as a mechanism that allows a model to focus on specific parts of the\n",
      "input when making predictions. In the context of the Transformer model\n",
      "described in the given information, attention is used to connect the\n",
      "encoder and decoder. It computes the compatibility between a query and\n",
      "a set of key-value pairs, and then uses this compatibility to assign\n",
      "weights to the values. These weighted values are then used to make\n",
      "predictions. The attention mechanism used in the Transformer model is\n",
      "called \"Scaled Dot-Product Attention\", which involves computing dot\n",
      "products between queries and keys, and then scaling the dot products\n",
      "by a factor of 1/sqrt(dk). This scaling helps to prevent the dot\n",
      "products from becoming too large and affecting the gradients during\n",
      "training.\n",
      "______________________________________________________________________\n",
      "Source Node 1/4\n",
      "Node ID: 8a58ccf2-81a7-4652-a57f-326bce8109fd\n",
      "Similarity: 0.8156380603711351\n",
      "Text: Attention Is All You Need Ashish Vaswani∗ Google Brain\n",
      "avaswani@google.comNoam Shazeer∗ Google Brain noam@google.comNiki\n",
      "Parmar∗ Google Research nikip@google.comJakob Uszkoreit∗ Google\n",
      "Research usz@google.com Llion Jones∗ Google Research\n",
      "llion@google.comAidan N. Gomez∗† University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗ Google Brain lukasz...\n",
      "______________________________________________________________________\n",
      "Source Node 2/4\n",
      "Node ID: 85eeddc4-64f4-4b9b-ba83-1e5aefb96f35\n",
      "Similarity: 0.8054754149096266\n",
      "Text: Scaled Dot-Product Attention  Multi-Head Attention Figure 2:\n",
      "(left) Scaled Dot-Product Attention. (right) Multi-Head Attention\n",
      "consists of several attention layers running in parallel. query with\n",
      "all keys, divide each by√dk, and apply a softmax function to obtain\n",
      "the weights on the values. In practice, we compute the attention\n",
      "function on a set ...\n",
      "______________________________________________________________________\n",
      "Source Node 3/4\n",
      "Node ID: 410cc296-18ba-4ebc-aabf-b296108fb329\n",
      "Similarity: 0.8041732963297556\n",
      "Text: [21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.\n",
      "Effective approaches to attention- based neural machine translation.\n",
      "arXiv preprint arXiv:1508.04025 , 2015. [22] Ankur Parikh, Oscar\n",
      "Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "model. In Empirical Methods in Natural Language Processing , 2016.\n",
      "[23] Romain Pa...\n",
      "______________________________________________________________________\n",
      "Source Node 4/4\n",
      "Node ID: c361de7d-832a-4be3-bf9a-751c93ae744d\n",
      "Similarity: 0.802995854955961\n",
      "Text: Figure 1: The Transformer - model architecture. wise fully\n",
      "connected feed-forward network. We employ a residual connection [ 10]\n",
      "around each of the two sub-layers, followed by layer normalization [\n",
      "1]. That is, the output of each sub-layer is LayerNorm( x+ Sublayer(\n",
      "x)), where Sublayer(x)is the function implemented by the sub-layer\n",
      "itself. To fa...\n",
      "The best way to understand attention is to think of it as a mechanism that allows a model to focus on specific parts of the input when making predictions. In the context of the Transformer model described in the given information, attention is used to connect the encoder and decoder. It computes the compatibility between a query and a set of key-value pairs, and then uses this compatibility to assign weights to the values. These weighted values are then used to make predictions. The attention mechanism used in the Transformer model is called \"Scaled Dot-Product Attention\", which involves computing dot products between queries and keys, and then scaling the dot products by a factor of 1/sqrt(dk). This scaling helps to prevent the dot products from becoming too large and affecting the gradients during training.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.response.pprint_utils  import pprint_response\n",
    "pprint_response(response, show_source=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3904f7c42c58b36cb379316da64e25217e7cd4359b222a6309c8a7bc6090891f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
