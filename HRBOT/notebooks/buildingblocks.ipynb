{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage1 - PDFreader with pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip show pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pdfplumber\n",
    "\n",
    "def pdf_folder_reader(folder_path):\n",
    "\n",
    "    pdf_text =\"\"\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            #read the pdf\n",
    "            with pdfplumber.open(file_path) as file:\n",
    "                for page in file.pages:\n",
    "                    pdf_text += page.extract_text()\n",
    "    return pdf_text\n",
    "folder_path = f\"../resources/uploads\"\n",
    "document = pdf_folder_reader(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage2 - text preprocessing\n",
    " - steps include:\n",
    "   1. Removing unwanted characters (dots, spaces, etc.).\n",
    "   2. Normalizing whitespace.\n",
    "   3. Handling line breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def preprocess_text(raw_text):\n",
    "    # Remove dots\n",
    "    text_without_dots = raw_text.replace('.', '.')\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text_normalized = ' '.join(text_without_dots.split())\n",
    "    pattern = re.compile(r'[^a-zA-Z0-9\\s]')\n",
    "    \n",
    "    # Use the sub method to replace matched characters with an empty string\n",
    "    cleaned_text = re.sub(pattern, '', text_normalized)\n",
    "\n",
    "    return cleaned_text \n",
    "\n",
    "cleaned_text = preprocess_text(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_special_characters(text):\n",
    "    # Define a regular expression to match any non-alphanumeric character\n",
    "    pattern = re.compile(r'[^a-zA-Z0-9\\s]')\n",
    "    \n",
    "    # Use the sub method to replace matched characters with an empty string\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    \n",
    "    return cleaned_text\n",
    "cleaned_text = remove_special_characters(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage3: chunking & embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Splitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "custom_separator = '\\n'\n",
    "custom_chunk_size = 5000\n",
    "custom_chunk_overlap = 50\n",
    "custom_paragraph_separator = '\\n\\n\\n'\n",
    "custom_regex = '[^,.;。？！]+[,.;。？！]?'\n",
    "\n",
    "\n",
    "custom_sentence_splitter = SentenceSplitter(\n",
    "    separator=custom_separator,\n",
    "    chunk_size=custom_chunk_size,\n",
    "    chunk_overlap=custom_chunk_overlap,\n",
    "    paragraph_separator=custom_paragraph_separator,    \n",
    ")\n",
    "\n",
    "# Split the text into chunks using custom parameters\n",
    "custom_chunks = custom_sentence_splitter.split_text(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the result\n",
    "for i, chunk in enumerate(custom_chunks, start=1):\n",
    "    print(f\"Chunk {i}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyDpBJeTmKU1yHCUxF-T85nxTi3dMxqbZTY'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_settings = [\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_ONLY_HIGH\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_ONLY_HIGH\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_ONLY_HIGH\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_ONLY_HIGH\"\n",
    "  },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SUBOMMAS\\LLM_Projects\\HRBOT\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "gemini_embedding = GeminiEmbedding(model_name=\"models/embedding-001\", api_key=GOOGLE_API_KEY)\n",
    "gemini = Gemini(model_name=\"models/gemini-pro\", temperature=1, max_tokens=2048, safety_settings=safety_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = gemini_embedding.get_text_embedding(custom_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Dimension of embeddings: {len(embeddings)}\")\n",
    "#print(embeddings[0][:5])\n",
    "#print(embeddings[1][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emdedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_embedding(text):\n",
    "    embeddings = gemini_embedding.get_text_embedding(text)\n",
    "    return embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for chunk in custom_chunks:\n",
    "  chunk_embedding = text_to_embedding(chunk)\n",
    "  embeddings.append(chunk_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings and Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(custom_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, chunk in enumerate(custom_chunks):\n",
    "  # Create a dictionary for each data point\n",
    "  data_point = {\n",
    "      #\"text\": chunk,  # Original text chunk\n",
    "      \"embedding\": embeddings[i],  # Corresponding embedding for the chunk\n",
    "      \"id\": str(f\"chunk_{i}\")  # Unique identifier for each chunk\n",
    "  }\n",
    "  data.append(data_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [item['id'] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embeddings = [item['embedding'] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, SimpleDirectoryReader, load_index_from_storage\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Llama global   Settings\n",
    "from llama_index.core import Settings\n",
    "\n",
    "#as default chroma db uses the open ai embeddings\n",
    "#setting up Gemini pro -llm and gemini embedding\n",
    "Settings.llm = gemini\n",
    "Settings.embed_model = gemini_embedding\n",
    "Settings.text_splitter = custom_sentence_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chorma PersistentClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "getting your data from wherever it lives, whether that’s unstructured text, PDFs, databases, or APIs to other applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"../resources/HR_Documents\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Storing\n",
    "Indexing is designed to enable querying by an LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Using Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk \n",
    "db = chromadb.PersistentClient(path=\"../resources/chroma_db\")   \n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "vector_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents, storage_context=vector_context, embed_model=gemini_embedding, transformations=[custom_sentence_splitter]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)\n",
    "#OR#\n",
    "#index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store using .persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSIST_DIR =\"../resources/persist\"\n",
    "persist_client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "persist_collection = persist_client.get_or_create_collection(\"persist\")\n",
    "\n",
    "persist_store = ChromaVectorStore(chroma_collection=persist_collection)\n",
    "persist_context = StorageContext.from_defaults(vector_store=persist_store)\n",
    "# load the data and create index\n",
    "index = VectorStoreIndex.from_documents(documents=documents, storage_context=persist_context, embed_model=gemini_embedding)\n",
    "index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the exiting index\n",
    "#storage_context= StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "index=load_index_from_storage(storage_context=persist_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Query Data from the stored vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 2 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>1. Vibhor Steel Tubes IPO is set for launch on 13 February 2024 with a price band of ₹141-151 per share.\n",
       "2. The company's strengths include its partnership with Jindal Pipes, experienced promoters, and a large workforce.\n",
       "3. Key risks to consider include the company's dependence on Jindal Pipes, negative cash flow from financing and investing activities, and volatile steel prices.\n",
       "4. The company's financial performance has shown growth in recent years, with an upward trend in profitability and key ratios such as Return on Equity (ROE).\n",
       "5. Vibhor Steel Tubes has the lowest Price to Earnings (P/E) ratio among its peers.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Without retriever\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"what are key points in context\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With COntextChat engine\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=3000)\n",
    "#VectorStoreIndex.refresh(vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"context\",\n",
    "    memory=memory,\n",
    "    system_prompt=(\n",
    "        \"You are a chatbot, able to have normal interactions, as well as talk and can greet\"\n",
    "        \" you are a chatbot well trained on IPO context and its working \"\n",
    "    ),\n",
    "\n",
    ")\n",
    "\n",
    "response = chat_engine.chat(\"when ipo is opening for bidding\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 8 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The details for the Vibhor Steel Tubes IPO are as follows:\n",
      "\n",
      "- IPO is scheduled from 13th to 15th February 2024.\n",
      "- Face value of ₹10 per share.\n",
      "- Price range is ₹141-152 per share.\n",
      "- Total IPO size is ₹72.17 crore.\n",
      "- No Offer for Sale (OFS) component.\n",
      "- Fresh issue of ₹72.17 crore.\n"
     ]
    }
   ],
   "source": [
    "#With retriever\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "#load index\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=1,\n",
    ")\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0)],\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"provide the ipo details?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Query Data from persisted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Vibhor Steel Tubes IPO opens for subscription from Feb 13 to Feb 15, 2024, with a price band of ₹141-152 per share. The IPO has a face value of ₹10 per share and comprises a fresh issue of ₹72.17 crores. The company’s strengths include a partnership with Jindal Pipes, diverse networking channels, and a workforce of 636 employees. However, the company faces risks related to negative cash flow from financing and investing activities, volatile steel prices, and continuous increase in total borrowing.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Without retriever\n",
    "index=load_index_from_storage(storage_context=persist_context)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Summarize the IPO\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IPO size is 72.17 crores.\n",
      "\n",
      "QIBs can invest 50% of this amount which is 36.085 crores\n",
      "\n",
      "NIIs can invest 35% of this amount which is 25.2595 crores\n",
      "\n",
      "Retail investors can invest 35% of this amount which is 25.2595 crores\n"
     ]
    }
   ],
   "source": [
    "#With retriever\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "#load index\n",
    "index = load_index_from_storage(storage_context=persist_context)\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=6,\n",
    ")\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0)],\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"how much of the ipo size and provide the investors portions for the ipo with how many crores can each group can invest?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default llama-index Vector Store\n",
    "Store your indexed data is to use the built-in `StorageContext.persist()` method of every Index, which writes all the data to disk at the location specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.storage.index_store import SimpleIndexStore\n",
    "from llama_index.core.vector_stores import SimpleVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# not specfic to chroma db\n",
    "PERSIST_DIR =\"../resources/persistenDB\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # create storage context using default stores\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        docstore=SimpleDocumentStore(),\n",
    "        vector_store=SimpleVectorStore(),\n",
    "        index_store=SimpleIndexStore(),\n",
    "    )\n",
    "    # load the data and create index\n",
    "    index = VectorStoreIndex.from_documents(documents=documents, embed_model=gemini_embedding)\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the exitiong index\n",
    "    storage_context= StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index=load_index_from_storage(storage_context=storage_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Vibhor Steel Tubes IPO, set to open for subscription from February 13 to 15, 2024, offers a price band of ₹141-152 per share. The IPO size is ₹72.17 crore, and the lot size is 99 shares. The issue comprises only fresh issue and has reserved shares for qualified institutional buyers (QIBs), non-institutional investors (NIIs), and retail investors.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "custom_qa_prompt_str=(\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query in the style of a Francis Beaumont play.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ") \n",
    "qa_prompt_tmpl = PromptTemplate(custom_qa_prompt_str)\n",
    "\n",
    "query_engine=index.as_query_engine(text_qa_template = qa_prompt_tmpl)\n",
    "\n",
    "query_engine=index.as_query_engine()\n",
    "prompts_dict = query_engine.get_prompts()\n",
    "#print(list(prompts_dict))\n",
    "\n",
    "response = query_engine.query(\"Summarize the IPO details\")\n",
    "Markdown(f\"<b>{response.response}</b>\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.source_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Chatbot Agent\n",
    "Testing the Agent\n",
    "https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/chatbots/building_a_chatbot.html\n",
    "\n",
    "Data Agents are LLM-powered knowledge workers in LlamaIndex that can intelligently perform various tasks over your data, in both a “read” and “write” function. They are capable of the following:\n",
    "\n",
    "Perform automated search and retrieval over different types of data - unstructured, semi-structured, and structured.\n",
    "\n",
    "Calling any external service API in a structured fashion, and processing the response + storing it for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ephemeral ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=\"../resources/manual\")\n",
    "#chroma_collection = chroma_client.create_collection(path=\"../resources/embeddingsDB\")\n",
    "\n",
    "# Create a Chroma collection\n",
    "chroma_collection = chroma_client.get_or_create_collection(name=\"embed_collecting\")\n",
    "\n",
    "\n",
    "# Add embeddings to the Chroma collection\n",
    "chroma_collection.upsert(ids=ids, embeddings=data_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk \n",
    "#vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "#storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chromadb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m Markdown\n\u001b[0;32m      2\u001b[0m \u001b[39m# load from disk\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m db2 \u001b[39m=\u001b[39m chromadb\u001b[39m.\u001b[39mPersistentClient(path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../resources/manual\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m chroma_collection \u001b[39m=\u001b[39m db2\u001b[39m.\u001b[39mget_or_create_collection(\u001b[39m\"\u001b[39m\u001b[39membed_collecting\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m vector_store \u001b[39m=\u001b[39m ChromaVectorStore(chroma_collection\u001b[39m=\u001b[39mchroma_collection)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chromadb' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "# load from disk\n",
    "db2 = chromadb.PersistentClient(path=\"../resources/manual\")\n",
    "chroma_collection = db2.get_or_create_collection(\"embed_collecting\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context= storage_context\n",
    ")\n",
    "#embeddings\n",
    "data = []\n",
    "for i, chunk in enumerate(custom_chunks):\n",
    "  # Create a dictionary for each data point\n",
    "  data_point = {\n",
    "      #\"text\": chunk,  # Original text chunk\n",
    "      \"embedding\": embeddings[i],  # Corresponding embedding for the chunk\n",
    "      \"id_\": str(f\"chunk_{i}\")  # Unique identifier for each chunk\n",
    "  }\n",
    "  data.append(data_point)\n",
    "\n",
    "index.insert(data)\n",
    "\n",
    "\n",
    "# Query Data from the persisted index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"explain few shift allowance benifts\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidDimensionException",
     "evalue": "Embedding dimension 3 does not match collection dimensionality 768",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidDimensionException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[39m=\u001b[39m chroma_collection\u001b[39m.\u001b[39;49mquery(\n\u001b[0;32m      2\u001b[0m     query_embeddings\u001b[39m=\u001b[39;49m[[\u001b[39m1.1\u001b[39;49m, \u001b[39m2.3\u001b[39;49m, \u001b[39m3.2\u001b[39;49m], [\u001b[39m5.1\u001b[39;49m, \u001b[39m4.3\u001b[39;49m, \u001b[39m2.2\u001b[39;49m]],\n\u001b[0;32m      3\u001b[0m     n_results\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[0;32m      4\u001b[0m     where\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mstyle\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mstyle2\u001b[39;49m\u001b[39m\"\u001b[39;49m}\n\u001b[0;32m      5\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\SUBOMMAS\\LLM_Projects\\HRBOT\\venv\\lib\\site-packages\\chromadb\\api\\models\\Collection.py:345\u001b[0m, in \u001b[0;36mCollection.query\u001b[1;34m(self, query_embeddings, query_texts, query_images, query_uris, n_results, where, where_document, include)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m include \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muris\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m include:\n\u001b[0;32m    344\u001b[0m     valid_include\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39muris\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 345\u001b[0m query_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49m_query(\n\u001b[0;32m    346\u001b[0m     collection_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mid,\n\u001b[0;32m    347\u001b[0m     query_embeddings\u001b[39m=\u001b[39;49mvalid_query_embeddings,\n\u001b[0;32m    348\u001b[0m     n_results\u001b[39m=\u001b[39;49mvalid_n_results,\n\u001b[0;32m    349\u001b[0m     where\u001b[39m=\u001b[39;49mvalid_where,\n\u001b[0;32m    350\u001b[0m     where_document\u001b[39m=\u001b[39;49mvalid_where_document,\n\u001b[0;32m    351\u001b[0m     include\u001b[39m=\u001b[39;49minclude,\n\u001b[0;32m    352\u001b[0m )\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    355\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m include\n\u001b[0;32m    356\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_loader \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[39mand\u001b[39;00m query_results[\u001b[39m\"\u001b[39m\u001b[39muris\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    358\u001b[0m ):\n\u001b[0;32m    359\u001b[0m     query_results[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [\n\u001b[0;32m    360\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_loader(uris) \u001b[39mfor\u001b[39;00m uris \u001b[39min\u001b[39;00m query_results[\u001b[39m\"\u001b[39m\u001b[39muris\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    361\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\SUBOMMAS\\LLM_Projects\\HRBOT\\venv\\lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    126\u001b[0m \u001b[39mif\u001b[39;00m trace_granularity \u001b[39m<\u001b[39m granularity:\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tracer:\n\u001b[0;32m    129\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SUBOMMAS\\LLM_Projects\\HRBOT\\venv\\lib\\site-packages\\chromadb\\api\\segment.py:674\u001b[0m, in \u001b[0;36mSegmentAPI._query\u001b[1;34m(self, collection_id, query_embeddings, n_results, where, where_document, include)\u001b[0m\n\u001b[0;32m    672\u001b[0m coll \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_collection(collection_id)\n\u001b[0;32m    673\u001b[0m \u001b[39mfor\u001b[39;00m embedding \u001b[39min\u001b[39;00m query_embeddings:\n\u001b[1;32m--> 674\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_dimension(coll, \u001b[39mlen\u001b[39;49m(embedding), update\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    676\u001b[0m metadata_reader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_manager\u001b[39m.\u001b[39mget_segment(collection_id, MetadataReader)\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m where \u001b[39mor\u001b[39;00m where_document:\n",
      "File \u001b[1;32mc:\\Users\\SUBOMMAS\\LLM_Projects\\HRBOT\\venv\\lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    126\u001b[0m \u001b[39mif\u001b[39;00m trace_granularity \u001b[39m<\u001b[39m granularity:\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tracer:\n\u001b[0;32m    129\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SUBOMMAS\\LLM_Projects\\HRBOT\\venv\\lib\\site-packages\\chromadb\\api\\segment.py:814\u001b[0m, in \u001b[0;36mSegmentAPI._validate_dimension\u001b[1;34m(self, collection, dim, update)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_collection_cache[\u001b[39mid\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mdimension\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dim\n\u001b[0;32m    813\u001b[0m \u001b[39melif\u001b[39;00m collection[\u001b[39m\"\u001b[39m\u001b[39mdimension\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m dim:\n\u001b[1;32m--> 814\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidDimensionException(\n\u001b[0;32m    815\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEmbedding dimension \u001b[39m\u001b[39m{\u001b[39;00mdim\u001b[39m}\u001b[39;00m\u001b[39m does not match collection dimensionality \u001b[39m\u001b[39m{\u001b[39;00mcollection[\u001b[39m'\u001b[39m\u001b[39mdimension\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m     )\n\u001b[0;32m    817\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    818\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[1;31mInvalidDimensionException\u001b[0m: Embedding dimension 3 does not match collection dimensionality 768"
     ]
    }
   ],
   "source": [
    "response = collection.query(\n",
    "    query_embeddings=[[1.1, 2.3, 3.2], [5.1, 4.3, 2.2]],\n",
    "    n_results=2,\n",
    "    where={\"style\": \"style2\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"index = VectorStoreIndex(\n",
    "    vector_store,\n",
    "    storage_context=StorageContext.in_memory(),\n",
    "    filename='vector_index.llama',\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The IPO dates are from 13th February 2024 to 15th February 2024.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "# load from disk\n",
    "db2 = chromadb.PersistentClient(path=\"../resources/manual\")\n",
    "chroma_collection = db2.get_or_create_collection(\"embed_collecting\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    embed_model= gemini_embedding,\n",
    ")\n",
    "\n",
    "# Query Data from the persisted index\n",
    "query_engine = index.as_query_engine()\n",
    "#response = query_engine.query(\"which is comapny that coming for ipo\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No index in storage context, check if you specified the right persist_dir.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39m\"\u001b[39m\u001b[39m../resources/manual\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m      3\u001b[0m     storage_context\u001b[39m=\u001b[39m StorageContext\u001b[39m.\u001b[39mfrom_defaults(vector_store\u001b[39m=\u001b[39mvector_store)\n\u001b[1;32m----> 4\u001b[0m     index\u001b[39m=\u001b[39mload_index_from_storage(storage_context\u001b[39m=\u001b[39;49mstorage_context)\n",
      "File \u001b[1;32mc:\\Users\\SUBOMMAS\\LLM_Projects\\HRBOT\\venv\\lib\\site-packages\\llama_index\\core\\indices\\loading.py:36\u001b[0m, in \u001b[0;36mload_index_from_storage\u001b[1;34m(storage_context, index_id, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m indices \u001b[39m=\u001b[39m load_indices_from_storage(storage_context, index_ids\u001b[39m=\u001b[39mindex_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(indices) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 36\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     37\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo index in storage context, check if you specified the right persist_dir.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m     )\n\u001b[0;32m     39\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(indices) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     40\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     41\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected to load a single index, but got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(indices)\u001b[39m}\u001b[39;00m\u001b[39m instead. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease specify index_id.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     43\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: No index in storage context, check if you specified the right persist_dir."
     ]
    }
   ],
   "source": [
    "#Defalut chroma\n",
    "if os.path.exists(\"../resources/manual\"):\n",
    "    storage_context= StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index=load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up ChromaVectorStore with the required config value 'chroma_server_host'\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Load data into the ChromaVectorStore\n",
    "#vector_store.load(collection=chroma_collection, field='embedding')\n",
    "\n",
    "# Create a vector index\n",
    "\n",
    "#storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, embed_model=gemini_embedding\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Data from the persisted index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"what are key points in context\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = chroma_collection.query(\n",
    "    query_texts=[\"what is this document about\"],\n",
    "    n_results=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [[]],\n",
       " 'distances': [[]],\n",
       " 'metadatas': [[]],\n",
       " 'embeddings': None,\n",
       " 'documents': [[]],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to use chunks to create vector index,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build index\n",
    "def build_index(dir, documents, key):\n",
    "    PERSIST_DIR = dir\n",
    "    persist_client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "    persist_collection = persist_client.get_or_create_collection(\"persist\")\n",
    "\n",
    "    persist_store = ChromaVectorStore(chroma_collection=persist_collection)\n",
    "    persist_context = StorageContext.from_defaults(vector_store=persist_store)\n",
    "    # load the data and create index\n",
    "    index = VectorStoreIndex.from_documents(documents=documents, storage_context=persist_context, embed_model=gemini_embedding)\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSIST_DIR = f\"C:/Users/SUBOMMAS/LLM_Projects/HRBOT/resources/finetuned2\"\n",
    "PERSIST_DIR =\"../resources/persist\"\n",
    "if os.path.exists(PERSIST_DIR):\n",
    "  index=load_index_from_storage(storage_context=persist_context)\n",
    "else:\n",
    " index = build_index(dir=PERSIST_DIR, documents=documents, key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Vibhor Steel Tubes IPO will take place between 13th and 15th February 2024, with a price band of ₹141-151 per share. The face value of each share is ₹10, and the total size of the IPO is ₹72.17 crores. The entire issue is a fresh issue, and no offer for sale is included.\n"
     ]
    }
   ],
   "source": [
    "query_engine=index.as_query_engine()\n",
    "response = query_engine.query(\"Summarize the IPO details\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface space to host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00e40789aa4b171b45271a1b239edfaef0c0abf7f6e57301bc673b8c54e742e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
